# ===================================================================
# CHR_class.py
#
# æ ¸å¿ƒåŠŸèƒ½ (Core Features):
#    - è‡ªå‹•åŒ–æ‰¹æ¬¡è™•ç†æ‰‹å¯«å­—ç·´ç¿’ç°¿çš„æƒæåœ–ç‰‡ã€‚
#    - ä¸‰é€šé“æ ¼å­åµæ¸¬ (è¼ªå»“åˆ†ææ³• / Hough ç›´ç·šè®Šæ› / æŠ•å½±å‰–é¢æ³•)ï¼Œ
#      è‡ªå‹•åˆ‡å‰²å‡ºæ¨™ç±¤åˆ—èˆ‡ç·´ç¿’æ ¼ã€‚
#    - OCR è¾¨è­˜æ¨™ç±¤æ ¼ï¼Œä¸¦æ”¯æ´ç™½åå–® (whitelist) èˆ‡æ™ºæ…§æ¨æ–·ï¼š
#        * ä½¿ç”¨è€…å¯æŒ‡å®šç¬¬ä¸€å€‹å­—ä½œç‚ºå…¨åŸŸéŒ¨é»ã€‚
#        * æä¾›æ¨¡å¼è¨˜æ†¶èˆ‡éŒ¨é»æ ¡æ­£ï¼Œæå‡åºåˆ—æ¨æ–·æº–ç¢ºåº¦ã€‚
#    - åš´æ ¼çš„ç©ºç™½æª¢æ¸¬ (å¤šç‰¹å¾µï¼šç­†ç•«æŒä¹…åº¦ / é‚Šç·£å¯†åº¦ / é€£é€šå…ƒä»¶)ï¼Œ
#      é¿å…å°‡ç©ºæ ¼æˆ–æ®˜å½±èª¤åˆ¤ç‚ºå­—è·¡ã€‚
#    - è‡ªå‹•è£åˆ‡èˆ‡åˆ†é¡å„²å­˜å­—è·¡åœ–ç‰‡ï¼Œä¸¦å»ºç«‹ä»¥å­—å…ƒç‚ºå–®ä½çš„è³‡æ–™å¤¾ã€‚
#    - ç”¢å‡ºè©³ç´°çµ±è¨ˆå ±å‘Šï¼š
#        * å„²å­˜ç‡ (å·²å­˜ / å¯¦éš›æœ‰å­—è·¡)
#        * è³‡æ–™ç”¢å‡ºç‡ (å·²å­˜ / ç†è«–æ ¼æ•¸ - ç©ºç™½ - æœªçŸ¥æ¨™ç±¤)
#        * ä½å­˜é‡æ¬„ä½å ±å‘Š (ä¸è¶³ 10 å€‹å­—è·¡çš„å­—å…ƒæœƒç‰¹åˆ¥æ¨™è¨»)
#    - Debug è¼”åŠ©ï¼šè¼¸å‡ºå¸¶æœ‰æ¨™è¨»æ¡†çš„æ¨™ç±¤åµæ¸¬åœ–ã€‚
#
# é‹è¡Œæµç¨‹ (Execution Flow):
#    1. åˆå§‹åŒ–ç’°å¢ƒï¼š
#         - å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾ (datasets/...) èˆ‡ debug è³‡æ–™å¤¾ã€‚
#         - è¼‰å…¥ whitelist.txtï¼Œè©¢å•æ˜¯å¦å•Ÿç”¨ç™½åå–®æ¨¡å¼ã€‚
#         - ä½¿ç”¨è€…å¯é¸æ“‡è¼¸å…¥ç¬¬ä¸€å€‹å­—å…ƒä½œç‚ºå…¨åŸŸéŒ¨é»ã€‚
#
#    2. é è™•ç†æª¢æŸ¥ï¼š
#         - è‹¥æœªæ‰¾åˆ°æ ¡æ­£å¾Œçš„ PNGï¼Œæœƒè‡ªå‹•å‘¼å« preprocess_pages.pyã€‚
#
#    3. æ ¼å­åµæ¸¬ï¼š
#         - ä¾åºå˜—è©¦ä¸‰ç¨®æ–¹æ³•æ‰¾å‡º 9Ã—10 çš„æ ¼å­çŸ©é™£ã€‚
#
#    4. æ¨™ç±¤è¾¨è­˜èˆ‡æ¨æ–·ï¼š
#         - Tesseract OCR å˜—è©¦è¾¨è­˜æ¯å€‹æ¨™ç±¤æ ¼ã€‚
#         - å•Ÿç”¨ç™½åå–®æ¨¡å¼æ™‚ï¼Œæœƒé€éåºåˆ—æ¯”å° / éŒ¨é»æ ¡æ­£ / æ¨¡å¼è¨˜æ†¶
#           ä¾†æ¨æ–·æœ€çµ‚æ¨™ç±¤ã€‚
#         - ç©ºç™½æˆ–ä¿¡å¿ƒåº¦ä¸è¶³çš„æ¨™ç±¤æœƒæ¨™è¨˜ç‚º '?'ã€‚
#
#    5. ç·´ç¿’æ ¼è™•ç†ï¼š
#         - è‹¥æ¨™ç±¤æ˜¯ '?' â†’ æ•´æ¬„ç·´ç¿’æ ¼è·³éã€‚
#         - å¦å‰‡é€æ ¼åˆ¤æ–·æ˜¯å¦ç‚ºç©ºç™½ï¼Œä¿ç•™æœ‰æ•ˆå­—è·¡ä¸¦è£åˆ‡å„²å­˜ã€‚
#
#    6. çµ±è¨ˆèˆ‡å ±å‘Šï¼š
#         - ç¸½é æ•¸ã€ç¸½æ ¼å­æ•¸ã€æ¨™ç±¤æ•¸ã€ç·´ç¿’æ ¼æ•¸ã€‚
#         - æˆåŠŸè¾¨è­˜çš„æ¨™ç±¤æ¬„ä½æ•¸ vs æœªçŸ¥æ¨™ç±¤æ¬„ä½æ•¸ã€‚
#         - å¯å®šå€æ ¼å­æ•¸ vs ç©ºç™½ vs å·²å„²å­˜æ•¸ã€‚
#         - å„²å­˜ç‡ã€è³‡æ–™ç”¢å‡ºç‡ã€‚
#         - ä½å­˜é‡æ¬„ä½è­¦å‘Šã€‚
#
# ===================================================================


import os
import cv2
import pytesseract
import subprocess
import sys
import numpy as np
import shutil
import unicodedata
import math

# --- 1. ç’°å¢ƒè¨­å®š ---

target_name = "250928"   # å¯ä»¥æ‰‹å‹•æŒ‡å®šï¼Œä¾‹å¦‚ "250928"ï¼›è‹¥ç•™ç©º "" å‰‡è™•ç† data/ ä¸‹æ‰€æœ‰å­è³‡æ–™å¤¾

pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
script_dir = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(script_dir, "data")
OUTPUT_DIR = os.path.join( "E:\datasets", target_name )
DEBUG_DIR = os.path.join(script_dir, "debug_steps")
WHITELIST_FILE = os.path.join(script_dir, "whitelist.txt")

# --- 2. ç¢ºå®šè¦è™•ç†çš„è³‡æ–™å¤¾ ---
if target_name:
    target_dirs = [os.path.join(DATA_DIR, target_name)]
    print(f"âœ” åƒ…è™•ç†æŒ‡å®šçš„å­è³‡æ–™å¤¾: {target_name}")
else:
    # æ‰¾å‡º data/ ä¸‹æ‰€æœ‰å­è³‡æ–™å¤¾
    target_dirs = [os.path.join(DATA_DIR, d) for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]
    print(f"âœ” æœªæŒ‡å®š target_nameï¼Œå°‡éæ­· data/ ä¸‹ {len(target_dirs)} å€‹å­è³‡æ–™å¤¾")

if not target_dirs:
    print("âŒ éŒ¯èª¤: æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„è³‡æ–™å¤¾")
    sys.exit(1)

# --- 3. éæ­·ä¸¦é‡æ–°å‘½å ---
all_page_files = []

for tdir in target_dirs:
    image_files = [f for f in os.listdir(tdir) if f.lower().endswith(('.jpg', '.png'))]
    image_files.sort()

    if not image_files:
        print(f"âš ï¸ è­¦å‘Š: {tdir} ä¸­æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè·³éã€‚")
        continue

    print(f"\nğŸ“‚ æ­£åœ¨è™•ç†è³‡æ–™å¤¾: {os.path.basename(tdir)}ï¼Œæ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")

    # é‡æ–°å‘½åç‚º 001.png, 002.png ...
    for idx, fname in enumerate(image_files, start=1):
        old_path = os.path.join(tdir, fname)
        new_name = f"{idx:03d}.png"  # çµ±ä¸€è½‰æˆ PNG
        new_path = os.path.join(tdir, new_name)

        # å¦‚æœåŸæœ¬å°±æ˜¯åŒå PNGï¼Œå‰‡è·³é rename
        if fname == new_name:
            all_page_files.append(new_path)
            continue

        img = cv2.imread(old_path)
        if img is None:
            print(f"  -> è­¦å‘Š: ç„¡æ³•è®€å– {old_path}ï¼Œè·³éã€‚")
            continue

        cv2.imwrite(new_path, img)
        os.remove(old_path)  # åˆªæ‰èˆŠæª”æ¡ˆï¼Œé¿å…é‡è¤‡
        all_page_files.append(new_path)
        print(f"  -> å·²é‡æ–°å‘½å {fname} â†’ {new_name}")

print(f"\nâœ” å…¨éƒ¨åœ–ç‰‡é‡æ–°ç·¨è™Ÿå®Œæˆï¼Œç¸½è¨ˆ {len(all_page_files)} å¼µå¯ä¾›åˆ†æã€‚")

# çµ±ä¸€åˆ†æä¾†æº (é€™è£¡ä¿ç•™åŸæœ¬ target æ¨¡å¼é‚è¼¯)
if target_name:
    PAGES_DIR = os.path.join(DATA_DIR, target_name)
else:
    # å¦‚æœè™•ç†å¤šå€‹è³‡æ–™å¤¾ï¼Œå°±æš«æ™‚ç”¨ç¬¬ä¸€å€‹ï¼Œå¾Œé¢æµç¨‹è‡ªå·±å¯éæ­· all_page_files
    PAGES_DIR = DATA_DIR

# --- 2. è®€å–ä¸¦è¼‰å…¥ç™½åå–® ---

use_whitelist = False
whitelist_text = ""
global_offset = None # ç”¨æ–¼å¾ªåºæ¨æ–·çš„å…¨åŸŸåç§»é‡
try:
    with open(WHITELIST_FILE, 'r', encoding='utf-8') as f:
        whitelist_text = "".join(f.read().split())
    print(f"âœ” æˆåŠŸè¼‰å…¥ {len(whitelist_text)} å€‹ç™½åå–®å­—å…ƒã€‚")

    choice = input("æ˜¯å¦å•Ÿç”¨ç™½åå–®æ¨æ–·åŠŸèƒ½ï¼Ÿ(ç›´æ¥æŒ‰ Enter è¡¨ç¤ºæ˜¯, è¼¸å…¥ n è¡¨ç¤ºå¦): ").strip().lower()
    if choice == 'n':
        use_whitelist = False
        print(" -> ç™½åå–®åŠŸèƒ½å·²åœç”¨ã€‚")
    else:
        use_whitelist = True
        print(" -> ç™½åå–®æ¨æ–·åŠŸèƒ½å·²å•Ÿç”¨ã€‚")
        
        first_char = input("è«‹è¼¸å…¥è©²æ‰¹è³‡æ–™é›†çš„ã€ç¬¬ä¸€å€‹å­—ã€‘(å¯ç•™ç©ºï¼Œç›´æ¥æŒ‰ Enter å‰‡æ¯é ç¨ç«‹å°‹æ‰¾éŒ¨é»): ").strip()
        if use_whitelist and first_char:
            # å°ä½¿ç”¨è€…è¼¸å…¥çš„å­—å…ƒä¹Ÿé€²è¡Œæ¨™æº–åŒ–ï¼Œä»¥ç¢ºä¿èƒ½æ­£ç¢ºåŒ¹é…
            normalized_first_char = unicodedata.normalize("NFKC", first_char)
            if normalized_first_char in whitelist_text:
                global_offset = whitelist_text.find(normalized_first_char)
                print(f"âœ” å·²è¨­å®šå…¨åŸŸèµ·å§‹éŒ¨é»ç‚º '{normalized_first_char}' (ç´¢å¼•: {global_offset})ï¼Œå°‡é€²è¡Œå¾ªåºæ¨æ–·ã€‚")
            else:
                print(f"âŒ è­¦å‘Š: èµ·å§‹å­— '{first_char}' ä¸åœ¨ç™½åå–®ä¸­ï¼Œå°‡é€€å›è‡³æ¯é ç¨ç«‹å°‹æ‰¾éŒ¨é»æ¨¡å¼ã€‚")

except FileNotFoundError:
    print(f"âŒ è­¦å‘Šï¼šæ‰¾ä¸åˆ°ç™½åå–®æª”æ¡ˆ '{WHITELIST_FILE}'ï¼Œç„¡æ³•ä½¿ç”¨ç™½åå–®åŠŸèƒ½ã€‚")
    use_whitelist = False

# --- 3. å»ºç«‹ä¸¦æ¸…ç©ºå¿…è¦è³‡æ–™å¤¾ ---

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DEBUG_DIR, exist_ok=True)
print("âœ” ç’°å¢ƒèˆ‡è³‡æ–™å¤¾è¨­å®šå®Œæˆã€‚")

print(f"æ¸…ç©ºèˆŠçš„ '{os.path.basename(OUTPUT_DIR)}' å’Œ '{os.path.basename(DEBUG_DIR)}' è³‡æ–™å¤¾...")
if os.path.isdir(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)
if os.path.isdir(DEBUG_DIR): shutil.rmtree(DEBUG_DIR)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DEBUG_DIR, exist_ok=True)

# ======================== æ ¸å¿ƒå·¥å…·å‡½å¼ =========================

def evaluate_grid_boxes(grid_boxes, expected_count=99, tol=15):
    """
    è©•ä¼°æ ¼å­åµæ¸¬çš„æ•ˆæœï¼Œå›å‚³åˆ†æ•¸
    - åˆ†æ•¸1: æ•¸é‡æ¥è¿‘åº¦ (è¶Šæ¥è¿‘ expected_count è¶Šå¥½)
    - åˆ†æ•¸2: è¦å‰‡æ€§ (æ ¼å­å¯¬é«˜ä¸€è‡´æ€§)
    """
    if not grid_boxes: 
        return -1
    
    count = len(grid_boxes)
    # åˆ†æ•¸1: æ•¸é‡æ¥è¿‘åº¦
    score_count = max(0, 1 - abs(count - expected_count) / expected_count)

    # åˆ†æ•¸2: æ’åˆ—è¦å‰‡æ€§ (å¯¬é«˜è®Šç•°åº¦è¶Šå°è¶Šå¥½)
    widths = [w for (_,_,w,h) in grid_boxes]
    heights = [h for (_,_,w,h) in grid_boxes]
    if not widths or not heights: 
        return score_count
    
    w_cv = np.std(widths) / (np.mean(widths)+1e-6)
    h_cv = np.std(heights) / (np.mean(heights)+1e-6)
    score_shape = max(0, 1 - (w_cv + h_cv))  

    return 0.7*score_count + 0.3*score_shape


def adaptive_find_grid_boxes(image):
    """
    ä¸‰é€šé“æ ¼å­åµæ¸¬ (Contours / Hough / Projection)
    è‡ªå‹•èª¿æ•´åƒæ•¸ä»¥æœ€å¤§åŒ–è¦†è“‹ç‡
    """
    best_boxes, best_score = [], -1
    params_candidates = [
        # min_area, max_area, min_ratio, max_ratio
        {'min_area': 35000, 'max_area': 65000, 'min_ratio': 0.85, 'max_ratio': 1.15, 'cluster_thresh': 30},
        {'min_area': 45000, 'max_area': 60000, 'min_ratio': 0.90, 'max_ratio': 1.10, 'cluster_thresh': 40},
        {'min_area': 30000, 'max_area': 70000, 'min_ratio': 0.80, 'max_ratio': 1.20, 'cluster_thresh': 50},
    ]

    # é€šé“ä¸€ Contours
    for p in params_candidates:
        boxes = find_grid_boxes_by_contours(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score:
            best_boxes, best_score = boxes, score

    # é€šé“äºŒ Hough
    for p in params_candidates:
        boxes = find_grid_boxes_by_hough(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score:
            best_boxes, best_score = boxes, score

    # é€šé“ä¸‰ Projection
    for p in params_candidates:
        boxes = find_grid_boxes_by_projection(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score:
            best_boxes, best_score = boxes, score

    print(f"  -> è‡ªé©æ‡‰æ ¼å­åµæ¸¬: æœ€ä½³æ ¼å­æ•¸ {len(best_boxes)}ï¼Œè©•åˆ† {best_score:.3f}")
    return best_boxes


def prepare_roi_for_ocr(full_img, box):
    """ç‚º OCR æº–å‚™é«˜å“è³ªçš„ ROIï¼ˆå¤šç‰ˆæœ¬äºŒå€¼åŒ–ï¼ŒæŒ‘ä¿¡å¿ƒåº¦æœ€ä½³çš„ï¼‰"""
    x, y, w, h = box
    roi = full_img[y:y+h, x:x+w]
    m = int(min(h, w) * 0.12)
    if m > 0 and h > 2*m and w > 2*m:
        roi = roi[m:h-m, m:w-m]

    g = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    g = cv2.bilateralFilter(g, 9, 50, 50)

    # CLAHE å¢å¼·
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    g = clahe.apply(g)

    # å¤šç‰ˆæœ¬äºŒå€¼åŒ–
    versions = []
    _, b1 = cv2.threshold(g, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    b2 = cv2.adaptiveThreshold(g, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                               cv2.THRESH_BINARY, 27, 10)
    b3 = cv2.adaptiveThreshold(g, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                               cv2.THRESH_BINARY, 27, 10)
    versions.extend([b1, b2, b3])

    # å˜—è©¦ OCRï¼ŒæŒ‘ä¿¡å¿ƒåº¦æœ€é«˜çš„ç‰ˆæœ¬
    best_img, best_conf = None, -1
    cfg = "--oem 3 --psm 8"
    for v in versions:
        if np.mean(v) < 127:  
            v = cv2.bitwise_not(v)
        v = cv2.copyMakeBorder(v, 20, 20, 20, 20,
                               cv2.BORDER_CONSTANT, value=255)

        try:
            data = pytesseract.image_to_data(
                v, lang='chi_tra', config=cfg, output_type=pytesseract.Output.DICT
            )
            confs = [int(c) for i, c in enumerate(data['conf'])
                     if int(c) > -1 and data['text'][i].strip()]
            mean_conf = float(np.mean(confs)) if confs else 0.0
            if mean_conf > best_conf:
                best_img, best_conf = v, mean_conf
        except:
            continue

    return best_img if best_img is not None else versions[0]



def ocr_char_and_conf(img_bin):
    """
    å°å–®ä¸€ ROI é€²è¡Œ OCRã€‚
    å¦‚æœ img_bin æ˜¯ listï¼Œæœƒå˜—è©¦å¤šç‰ˆæœ¬ï¼Œå–ä¿¡å¿ƒåº¦æœ€é«˜çš„çµæœã€‚
    å›å‚³ (å­—å…ƒ, ä¿¡å¿ƒåº¦)
    """
    cfg = "--oem 3 --psm 8"  # æ”¹æˆæ›´ç©©å®šçš„çµ„åˆ
    candidates = []

    # å¦‚æœæ˜¯å–®ä¸€å½±åƒï¼ŒåŒ…æˆ list çµ±ä¸€è™•ç†
    if not isinstance(img_bin, list):
        img_bin = [img_bin]

    for img in img_bin:
        try:
            data = pytesseract.image_to_data(
                img, lang='chi_tra', config=cfg, output_type=pytesseract.Output.DICT
            )
            confs = [int(c) for i, c in enumerate(data['conf']) if int(c) > -1 and data['text'][i].strip()]
            text = "".join(t for t in data['text'] if t.strip())
            char = "".join(c for c in text if '\u4e00' <= c <= '\u9fff')
            final_char = char[0] if char else ""
            mean_conf = float(np.mean(confs)) if confs else 0.0
            candidates.append((final_char, mean_conf))
        except Exception as e:
            print(f"âš ï¸ OCR éŒ¯èª¤: {e}")
            continue

    if not candidates:
        return "", 0.0
    # å–ä¿¡å¿ƒåº¦æœ€é«˜çš„ç‰ˆæœ¬
    return max(candidates, key=lambda x: x[1])


def _persistence_mask(gray, ksizes=(25, 41), min_keep=2):
    """
    å¤šé‡äºŒå€¼åŒ–çš„ 'æŒä¹…åº¦' ä¼°è¨ˆï¼šæŠŠå¤šç¨®é–€æª»çš„çµæœåš AND/ORï¼Œæ¯”è¼ƒç©©å®šçš„å¢¨æ°´æ‰ç®—æ•¸ã€‚
    å›å‚³ (persistence_ratio, union_ratio)
    """
    # ä¸‰ç¨®äºŒå€¼åŒ–ï¼šOtsu + å…©ç¨® adaptive mean (ä¸åŒ window)
    _, otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    ams = []
    for k in ksizes:
        k = max(15, k | 1)  # å¥‡æ•¸
        am = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, k, 10)
        ams.append(am)
    stack = [otsu] + ams
    union = np.zeros_like(otsu)
    votes = np.zeros_like(otsu, dtype=np.uint8)
    for m in stack:
        union = cv2.bitwise_or(union, m)
        votes = cv2.add(votes, (m > 0).astype(np.uint8))
    keep = (votes >= min_keep).astype(np.uint8) * 255
    inter = cv2.bitwise_and(union, keep)

    inter_cnt = int(cv2.countNonZero(inter))
    union_cnt = int(cv2.countNonZero(union))
    persistence_ratio = (inter_cnt / union_cnt) if union_cnt > 0 else 0.0
    return persistence_ratio, (union_cnt / (gray.shape[0] * gray.shape[1]) if gray.size else 0.0)


def _stroke_stats(gray):
    """
    é‚Šç·£ + é€£é€šå…ƒä»¶çš„ç­†ç•«çµ±è¨ˆã€‚å›å‚³ (edge_density, n_cc, max_cc_area_ratio)
    """
    # é‚Šç·£
    edges = cv2.Canny(gray, 60, 180)
    edge_density = float(np.count_nonzero(edges)) / float(edges.size)

    # é€£é€šå…ƒä»¶ï¼ˆåœ¨æ¯”è¼ƒä¹¾æ·¨çš„äºŒå€¼åœ–ä¸Šï¼‰
    _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    # æ¶ˆé™¤é¹½èƒ¡æ¤’
    bw = cv2.morphologyEx(bw, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=1)
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(bw, connectivity=8)
    # å»æ‰èƒŒæ™¯ï¼Œä¸¦å¿½ç•¥å¾®å°é›œè¨Š
    areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels) if stats[i, cv2.CC_STAT_AREA] >= 20]
    n_cc = len(areas)
    max_cc = max(areas) if areas else 0
    max_cc_area_ratio = max_cc / float(gray.size)
    return edge_density, n_cc, max_cc_area_ratio


def is_label_blank_ultra_strict(gray,
                                std_thresh = 20,
                                union_ink_ratio_min=0.040,
                                persistence_min=0.70,
                                edge_density_min=0.012,
                                n_cc_min=1,
                                max_cc_area_ratio_min=0.005):
    if gray is None or gray.size == 0:
        return True

    if np.std(gray) < std_thresh:
        return True

    persis, union_ratio = _persistence_mask(gray)
    if union_ratio < union_ink_ratio_min:
        return True
    if persis < persistence_min:
        return True

    edge_density, n_cc, max_cc_area_ratio = _stroke_stats(gray)
    if edge_density < edge_density_min:
        return True
    if n_cc < n_cc_min:
        return True
    if max_cc_area_ratio < max_cc_area_ratio_min:
        return True

    return False


def is_grid_blank_dynamically(gray,
                              std_thresh=25,
                              union_ink_ratio_min=0.020,
                              persistence_min=0.60,
                              edge_density_min=0.010,
                              n_cc_min=1,
                              max_cc_area_ratio_min=0.004):
    """
    åŸºæ–¼å¤šé‡ç‰¹å¾µçš„å‹•æ…‹ç©ºç™½æª¢æŸ¥ï¼Œå°ˆç‚ºæ‰‹å¯«å­—è·¡è¨­è¨ˆã€‚
    æ­¤æ©Ÿåˆ¶èˆ‡æ¨™ç±¤åˆ—çš„ is_label_blank_ultra_strict é‚è¼¯ç›¸åŒï¼Œä½†é–¥å€¼é‡å°æ‰‹å¯«ç‰¹æ€§èª¿æ•´ã€‚
    æ ¸å¿ƒæ˜¯é€éç­†ç•«æŒä¹…åº¦ã€é‚Šç·£å¯†åº¦ã€é€£é€šå…ƒä»¶ç­‰å¤šç¶­åº¦ç‰¹å¾µä¾†åˆ¤æ–·ã€Œç­†ç•«é‡ã€ï¼Œ
    è€Œéå–®ç´”çš„å¢¨æ°´æ¯”ä¾‹ï¼Œèƒ½æ›´æº–ç¢ºåœ°åˆ†è¾¨å¾®å¼±/æ½¦è‰å­—è·¡èˆ‡ç´”ç²¹çš„é›œè¨Š/ç©ºç™½ã€‚
    """
    if gray is None or gray.size == 0:
        return True

    if np.std(gray) < std_thresh:
        return True

    persis, union_ratio = _persistence_mask(gray)
    if union_ratio < union_ink_ratio_min:
        return True
    if persis < persistence_min:
        return True

    edge_density, n_cc, max_cc_area_ratio = _stroke_stats(gray)
    if edge_density < edge_density_min:
        return True
    if n_cc < n_cc_min:
        return True
    if max_cc_area_ratio < max_cc_area_ratio_min:
        return True

    return False

def find_grid_boxes_by_contours(image, params):
    """é€šé“ä¸€ï¼šè¼ªå»“åˆ†ææ³•"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    denoised_gray = cv2.medianBlur( gray, 3 )

    thresh = cv2.adaptiveThreshold( denoised_gray, 
                                      255 , cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 
                                      27, 23 )
    erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, ( 5, 1 ))
    eroded_thresh = cv2.erode( thresh, erode_kernel, iterations = 1 )
    close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, ( 8, 5 ))
    fixed_thresh = cv2.morphologyEx(eroded_thresh, cv2.MORPH_CLOSE, close_kernel, iterations = 2 )
    contours, _ = cv2.findContours(fixed_thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

    grid_boxes = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area, aspect_ratio = w * h, w / h if h > 0 else 0
        if params['min_area'] < area < params['max_area'] and params['min_ratio'] < aspect_ratio < params['max_ratio']:
            grid_boxes.append((x, y, w, h))
    return grid_boxes

def find_grid_boxes_by_hough(image, params):
    """é€šé“äºŒï¼šéœå¤«ç›´ç·šè®Šæ›æ³• (Fallback)"""
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    denoised_gray = cv2.medianBlur( gray, 3) 
    edges = cv2.Canny( denoised_gray, 25, 80, apertureSize = 3 )
    lines = cv2.HoughLinesP( edges, 1, np.pi / 180, 
                             threshold = 110, 
                             minLineLength = int( image.shape[0]//4 ),
                             maxLineGap = 50 )

    if lines is None: return []

    def cluster_lines(lines, threshold=params['cluster_thresh']):
        if not lines: return []
        lines = sorted(lines)
        clusters, current_cluster = [], [lines[0]]
        for line_pos in lines[1:]:
            if line_pos - current_cluster[-1] < threshold: current_cluster.append(line_pos)
            else:
                clusters.append(int(np.mean(current_cluster)))
                current_cluster = [line_pos]
        clusters.append(int(np.mean(current_cluster)))
        return clusters

    horz_lines, vert_lines = [], []
    for line in lines:
        x1, y1, x2, y2 = line[0]
        angle = abs(math.degrees(math.atan2(y2 - y1, x2 - x1))) if x2 - x1 != 0 else 90
        if angle < 15 or angle > 165: horz_lines.append(y1)
        elif 75 < angle < 105: vert_lines.append(x1)

    h_lines = cluster_lines(horz_lines)
    v_lines = cluster_lines(vert_lines)

    hough_boxes = []
    if len(h_lines) > 1 and len(v_lines) > 1:
        for i in range(len(h_lines) - 1):
            for j in range(len(v_lines) - 1):
                y1, y2 = h_lines[i], h_lines[i+1]
                x1, x2 = v_lines[j], v_lines[j+1]
                w, h = x2 - x1, y2 - y1
                if w > 50 and h > 50: hough_boxes.append((x1, y1, w, h))
    return hough_boxes

def find_grid_boxes_by_projection(image, params):
    """é€šé“ä¸‰ï¼šæŠ•å½±å‰–é¢æ³• (æœ€çµ‚ç‹ç‰Œ)"""
    print("  -> [é€šé“ 3] åŸ·è¡ŒæŠ•å½±å‰–é¢æ³•...")
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 21, 10)

    horz_proj, vert_proj = np.sum(binary, axis=1), np.sum(binary, axis=0)

    def find_peaks(projection, min_dist, threshold_ratio=0.3):
        threshold = np.max(projection) * threshold_ratio
        peaks = []
        for i in range(1, len(projection) - 1):
            if projection[i] > threshold and projection[i] > projection[i-1] and projection[i] > projection[i+1]:
                if all(abs(i - p) >= min_dist for p in peaks):
                    peaks.append(i)
        return peaks

    avg_grid_side = (params['min_area']**0.5 + params['max_area']**0.5) / 2

    y_coords = find_peaks(horz_proj, int(avg_grid_side * 0.8))
    x_coords = find_peaks(vert_proj, int(avg_grid_side * 0.8))

    proj_boxes = []
    if len(y_coords) > 1 and len(x_coords) > 1:
        for i in range(len(y_coords) - 1):
            for j in range(len(x_coords) - 1):
                y1, y2, x1, x2 = y_coords[i], y_coords[i+1], x_coords[j], x_coords[j+1]
                proj_boxes.append((x1, y1, x2-x1, y2-y1))
    return proj_boxes

def find_grid_boxes(image):
    """
    ä¸‰é€šé“æ ¼å­åµæ¸¬ (Contours / Hough / Projection)
    æ¯å€‹é€šé“å…§è‡ªå‹•æŒ‘å‡ºæœ€ä½³åƒæ•¸ï¼Œå†ä¸‰é€šé“æ¯”è¼ƒï¼Œè¼¸å‡ºæœ€çµ‚æœ€ä½³çµæœ
    """
    params_candidates = [
        {'min_area': 35000, 'max_area': 65000, 'min_ratio': 0.85, 'max_ratio': 1.15, 'cluster_thresh': 30},
        {'min_area': 45000, 'max_area': 60000, 'min_ratio': 0.90, 'max_ratio': 1.10, 'cluster_thresh': 40},
        {'min_area': 30000, 'max_area': 70000, 'min_ratio': 0.80, 'max_ratio': 1.20, 'cluster_thresh': 50},
    ]

    # --- é€šé“ä¸€ Contours ---
    print("  -> [é€šé“ 1] è¼ªå»“åˆ†ææ³•å˜—è©¦ä¸­...")
    best_contours, best_score_contours = [], -1
    for p in params_candidates:
        boxes = find_grid_boxes_by_contours(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score_contours:
            best_contours, best_score_contours = boxes, score
    print(f"     æœ€ä½³åƒæ•¸ä¸‹æ‰¾åˆ° {len(best_contours)} æ ¼å­ (score={best_score_contours:.3f})")

    # --- é€šé“äºŒ Hough ---
    print("  -> [é€šé“ 2] Hough ç›´ç·šè®Šæ›æ³•å˜—è©¦ä¸­...")
    best_hough, best_score_hough = [], -1
    for p in params_candidates:
        boxes = find_grid_boxes_by_hough(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score_hough:
            best_hough, best_score_hough = boxes, score
    print(f"     æœ€ä½³åƒæ•¸ä¸‹æ‰¾åˆ° {len(best_hough)} æ ¼å­ (score={best_score_hough:.3f})")

    # --- é€šé“ä¸‰ Projection ---
    print("  -> [é€šé“ 3] æŠ•å½±å‰–é¢æ³•å˜—è©¦ä¸­...")
    best_proj, best_score_proj = [], -1
    for p in params_candidates:
        boxes = find_grid_boxes_by_projection(image, p)
        score = evaluate_grid_boxes(boxes)
        if score > best_score_proj:
            best_proj, best_score_proj = boxes, score
    print(f"     æœ€ä½³åƒæ•¸ä¸‹æ‰¾åˆ° {len(best_proj)} æ ¼å­ (score={best_score_proj:.3f})")

    # --- ä¸‰é€šé“æœ€çµ‚æ¯”è¼ƒ ---
    candidates = [
        ("Contours", best_contours, best_score_contours),
        ("Hough", best_hough, best_score_hough),
        ("Projection", best_proj, best_score_proj)
    ]
    best_method, best_boxes, best_score = max(candidates, key=lambda x: x[2])

    print(f"  -> âœ… æœ€çµ‚æ¡ç”¨ {best_method}ï¼Œå…± {len(best_boxes)} å€‹æ ¼å­ (score={best_score:.3f})")
    return best_boxes

# ========================= è‡ªå‹•åŒ–é è™•ç†æª¢æŸ¥ =========================

if not os.path.isdir(PAGES_DIR) or not os.listdir(PAGES_DIR):
    print(f"\nâ„¹ï¸ æç¤ºï¼šæ‰¾ä¸åˆ°æˆ– '{os.path.basename(PAGES_DIR)}' è³‡æ–™å¤¾æ˜¯ç©ºçš„ã€‚")
    print(f"å°‡è‡ªå‹•åŸ·è¡Œ 'preprocess_pages.py'...")
    preprocess_script_path = os.path.join(script_dir, 'preprocess_pages.py')
    if not os.path.isfile(preprocess_script_path):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° {preprocess_script_path}"); exit()
    try:
        env = os.environ.copy()
        env["PYTHONUTF8"] = "1"
        result = subprocess.run(
            [sys.executable, preprocess_script_path],
            check=True, capture_output=True, text=True, encoding='utf-8', env=env
        )
        print("\n--- preprocess_pages.py æ—¥èªŒ ---\n", result.stdout, "\n--- æ—¥èªŒçµæŸ ---")
    except subprocess.CalledProcessError as e:
        print("\nâŒ éŒ¯èª¤ï¼šåŸ·è¡Œé è™•ç†å¤±æ•—\n", e.stderr); exit()
    print("\nâœ” é è™•ç†å®Œæˆï¼Œç¹¼çºŒåŸ·è¡Œåˆ†æ...")
else:
    print(f"\nâœ” åµæ¸¬åˆ° '{os.path.basename(PAGES_DIR)}' å·²å­˜åœ¨ï¼Œç›´æ¥é€²å…¥åˆ†æã€‚")

# ========================= ä¸»è™•ç†è¿´åœˆ =========================

print("\nâ³ é–‹å§‹åˆ†æå·²æ ¡æ­£çš„åœ–ç‰‡ä¸¦åˆ‡å‰²å­—è·¡...")
char_counters = {}
page_files = sorted([f for f in os.listdir(PAGES_DIR) if f.endswith('.png')])   ## é è¨­ã€€PNG æª”

# å…¨å±€çµ±è¨ˆè®Šæ•¸
total_pages_processed = 0
total_grids_found = 0
total_label_boxes_found = 0
total_practice_grids_found = 0
total_labels_recognized = 0
total_handwriting_saved = 0
total_blanks_skipped = 0
total_addressable_grids = 0
incomplete_columns_log = []
GRIDS_PER_PAGE_THEORY = 9 * 10

# <<< æ–°å¢ï¼šæ¨¡å¼è³‡æ–™åº« >>>
pattern_database = []
last_known_anchor_index = -1

for page_idx, page_filename in enumerate(page_files):
    print(f"\n--- æ­£åœ¨åˆ†æé é¢: {page_filename} ({page_idx + 1}/{len(page_files)}) ---")
    image = cv2.imread(os.path.join(PAGES_DIR, page_filename))
    if image is None:
        print(f"  -> ç„¡æ³•è®€å–åœ–ç‰‡ {page_filename}ï¼Œè·³éã€‚")
        continue

    # === æ”¾å¤§é è™•ç† ===
    SCALE_FACTOR = 1.55   # å¯ä»¥èª¿æ•´ 1.2 ~ 2.0 ä¹‹é–“ï¼Œå»ºè­°ä¸è¦å¤ªå¤§
    image = cv2.resize(image, None, fx=SCALE_FACTOR, fy=SCALE_FACTOR, interpolation=cv2.INTER_CUBIC)

    total_pages_processed += 1

    # === æ­¥é©Ÿ 1: ä¸‰é€šé“æ ¼å­åµæ¸¬ ===
    grid_boxes = find_grid_boxes(image)
    if len(grid_boxes) < 9:
        print(f"  -> è­¦å‘Š: æ‰¾åˆ°çš„æ ¼å­æ•¸ ({len(grid_boxes)}) éå°‘ï¼Œè·³éæ­¤é ã€‚"); continue
    total_grids_found += len(grid_boxes)
    grid_boxes.sort(key=lambda b: (b[1], b[0]))

    # === æ­¥é©Ÿ 2: æ¨™ç±¤åˆ—å®šä½ (æ ¹æ“šå›ºå®šè¦å‰‡) ===
    COL_COUNT = 9

    # åˆ†é›¢æ¨™ç±¤åˆ—èˆ‡ç·´ç¿’æ ¼
    first_row_boxes = grid_boxes[:COL_COUNT]
    practice_boxes = grid_boxes[COL_COUNT:]

    first_row_boxes.sort(key=lambda b: b[0])
        
    print(f"  -> å·²ç¢ºå®šæ¨™ç±¤åˆ—: {len(first_row_boxes)} å€‹, ç·´ç¿’æ ¼: {len(practice_boxes)} å€‹")

    total_label_boxes_found += len(first_row_boxes)
    total_practice_grids_found += len(practice_boxes)

    # === æ­¥é©Ÿ 3: æ¨™ç±¤ OCR èˆ‡æ¨¡å¼æ¨æ–· ===
    final_labels = ["?"] * len(first_row_boxes)
    is_determined = False

    # --- åˆæ­¥ OCR ---
    ocr_results = []
    for i, label_box in enumerate(first_row_boxes):
        roi_bin = prepare_roi_for_ocr(image, label_box)
        ch, conf = ocr_char_and_conf(roi_bin)

        if ch and conf < 45:
            ch = None

        x, y, w, h = label_box
        label_roi = image[y:y+h, x:x+w]
        gray_label_roi = cv2.cvtColor(label_roi, cv2.COLOR_BGR2GRAY)
        if is_label_blank_ultra_strict(gray_label_roi):
            ch = None

        ocr_results.append(ch)

    print(f"  -> åˆæ­¥ OCR çµæœ: [{' '.join(c if c else '?' for c in ocr_results)}]")

    # --- é å…ˆè™•ç†ç©ºç™½æ¨™ç±¤æ ¼ ---
    blanks_in_label_row = 0
    for i, label_box in enumerate(first_row_boxes):
        x, y, w, h = label_box
        label_roi = image[y:y+h, x:x+w]
        gray_label_roi = cv2.cvtColor(label_roi, cv2.COLOR_BGR2GRAY)
        if is_label_blank_ultra_strict(gray_label_roi):
            ocr_results[i] = None
            blanks_in_label_row += 1
    if blanks_in_label_row > 0:
        print(f"  -> åµæ¸¬åˆ°æ¨™ç±¤åˆ—æœ‰ {blanks_in_label_row} å€‹ç©ºç™½æ ¼ï¼Œå·²åœ¨åºåˆ—é…å°ä¸­æ¨™è¨˜ç‚ºå¿½ç•¥ã€‚")


    # --- æ¨æ–·æµç¨‹é–‹å§‹ ---
    if use_whitelist and whitelist_text:
        if page_idx == 0 and global_offset is not None:
            print(f"  -> [å„ªå…ˆå±¤ 1] ä½¿ç”¨è€…æŒ‡å®šèµ·å§‹å­—å…ƒï¼Œå¼·åˆ¶è¨­å®šåç§»é‡ç‚º {global_offset}")
            for i in range(len(first_row_boxes)):
                inferred_idx = global_offset + i
                if 0 <= inferred_idx < len(whitelist_text):
                    final_labels[i] = whitelist_text[inferred_idx]
            is_determined = True
            last_known_anchor_index = global_offset
        
        if not is_determined and pattern_database:
            print(f"  -> [å„ªå…ˆå±¤ 2] å˜—è©¦åŒ¹é… {len(pattern_database)} å€‹å·²è¨˜æ†¶çš„åºåˆ—...")
            for pattern in pattern_database:
                if len(pattern) != len(ocr_results): continue
                matches = sum(1 for i in range(len(ocr_results)) if ocr_results[i] and ocr_results[i] == pattern[i])
                if matches >= len(first_row_boxes) // 2:
                    final_labels = pattern
                    is_determined = True
                    try:
                        first_valid_char = next(c for c in final_labels if c != '?')
                        last_known_anchor_index = whitelist_text.find(first_valid_char)
                    except StopIteration:
                        pass
                    print(f"  -> âœ”ï¸ æ¨¡å¼åŒ¹é…æˆåŠŸï¼å¥—ç”¨å·²çŸ¥åºåˆ—: [{' '.join(final_labels)}]")
                    break
        
        if not is_determined:
            print(f"  -> [å„ªå…ˆå±¤ 3] æ¨¡å¼åŒ¹é…å¤±æ•—ï¼Œå•Ÿç”¨æ™ºæ…§éŒ¨é»æ¨æ–·...")
            
            label_results = []
            for i, label_box in enumerate(first_row_boxes):
                ch = ocr_results[i]
                idx, conf = None, -1
                if ch:
                    temp_roi_bin = prepare_roi_for_ocr(image, label_box)
                    _, conf = ocr_char_and_conf(temp_roi_bin)
                    
                    normalized_ch = unicodedata.normalize("NFKC", ch)
                    if normalized_ch in whitelist_text:
                        idx = whitelist_text.find(normalized_ch)
                label_results.append({'pos': i, 'char': ch, 'conf': conf, 'idx': idx})

            anchors = [res for res in label_results if res['idx'] is not None and res['conf'] >= 40.0]
            final_offset = None

            # --- é †åº 3a: å®¹éŒ¯åºåˆ—é…å° (å„ªå…ˆ) ---
            MIN_MATCH_COUNT = 3
            best_score = -1
            best_offset = None

            if len(ocr_results) > 0 and len(whitelist_text) >= len(ocr_results):
                for offset in range(len(whitelist_text) - len(ocr_results) + 1):
                    current_score = 0
                    for i in range(len(ocr_results)):
                        if ocr_results[i] and ocr_results[i] == whitelist_text[offset + i]:
                            current_score += 1
                    
                    if current_score > best_score:
                        best_score = current_score
                        best_offset = offset

            if best_score >= MIN_MATCH_COUNT:
                final_offset = best_offset
                print(f"        -> [å±¤ç´š 3a] æˆåŠŸï¼šåºåˆ—é…å°æ‰¾åˆ°æœ€ä½³å»åˆä½ç½® (ä½ç§»: {final_offset}, åŒ¹é…æ•¸: {best_score}/{len(ocr_results) - blanks_in_label_row})")
            else:
                print(f"        -> [å±¤ç´š 3a] å¤±æ•—ï¼šæœªæ‰¾åˆ°è¶³å¤ çš„åŒ¹é… (æœ€é«˜åƒ… {best_score} å€‹)ï¼Œç„¡æ³•ç¢ºå®šåºåˆ—ã€‚")

            # --- é †åº 3b: é«˜ä¿¡å¿ƒåº¦å–®é»éŒ¨å®š (æ¬¡ä¹‹) ---
            if final_offset is None and anchors:
                best_anchor = max(anchors, key=lambda a: a['conf'])
                HIGH_CONF_THRESHOLD = 85.0
                if best_anchor['conf'] >= HIGH_CONF_THRESHOLD:
                    final_offset = best_anchor['idx'] - best_anchor['pos']
                    print(f"        -> [å±¤ç´š 3b] æˆåŠŸï¼šæ¡ç”¨é«˜ä¿¡å¿ƒåº¦éŒ¨é» '{best_anchor['char']}' (ä¿¡å¿ƒåº¦: {best_anchor['conf']:.0f}%)")
                else:
                    print(f"        -> [å±¤ç´š 3b] å¤±æ•—ï¼šç„¡éŒ¨é»ä¿¡å¿ƒåº¦è¶…é {HIGH_CONF_THRESHOLD}%ã€‚")

            # --- é †åº 3c: ä¸­ä½æ•¸åç§»é‡æ ¡æ­£ (Fallback) ---
            if final_offset is None:
                if anchors:
                    offsets = [anchor['idx'] - anchor['pos'] for anchor in anchors]
                    final_offset = int(np.median(offsets))
                    print(f"        -> [å±¤ç´š 3c] æ¡ç”¨ä¸­ä½æ•¸åç§»é‡é€²è¡Œæ ¡æ­£: {final_offset}")
                else:
                    print(f"        -> [å±¤ç´š 3c] å¤±æ•—ï¼šæ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•ˆéŒ¨é»ã€‚")
            
            # --- å¥—ç”¨æœ€çµ‚è¨ˆç®—å‡ºçš„åç§»é‡ ---
            if final_offset is not None:
                temp_labels = ["?"] * len(first_row_boxes)
                for i in range(len(first_row_boxes)):
                    inferred_idx = i + final_offset
                    if 0 <= inferred_idx < len(whitelist_text):
                        temp_labels[i] = whitelist_text[inferred_idx]
                
                if any(label != "?" for label in temp_labels):
                    final_labels = temp_labels
                    if final_labels not in pattern_database:
                        pattern_database.append(final_labels)
                        print(f"  -> âœ”ï¸ éŒ¨é»æ¨æ–·æˆåŠŸï¼å­¸ç¿’åˆ°æ–°æ¨¡å¼: [{' '.join(final_labels)}]")
                    last_known_anchor_index = final_offset

    # å¦‚æœæ¨™ç±¤æ˜¯é å…ˆåˆ¤å®šçš„ç©ºç™½ï¼Œå³ä½¿æ¨æ–·å‡ºäº†çµæœï¼Œä¹Ÿå¼·åˆ¶å…¶ç‚º '?'
    for i in range(len(ocr_results)):
        if ocr_results[i] is None:
            final_labels[i] = '?'

    if not any(label != "?" for label in final_labels):
        original_ocr_results = [item if item is not None else '?' for item in ocr_results]
        print("  -> æœªå•Ÿç”¨ç™½åå–®æˆ–æ‰€æœ‰æ¨æ–·å¤±æ•—ï¼Œä½¿ç”¨å–®å­—è¾¨è­˜çµæœã€‚")
        final_labels = original_ocr_results

    total_labels_recognized += sum(1 for lab in final_labels if lab != "?")

    debug_img = image.copy()
    for i, (x, y, w, h) in enumerate(first_row_boxes):
        label_text = final_labels[i]
        cv2.rectangle(debug_img, (x, y), (x + w, y + h), (0, 0, 255), 3)
        cv2.putText(debug_img, label_text, (x, y - 15), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3 )
    cv2.imwrite(os.path.join(DEBUG_DIR, f'{page_filename}_labels.png'), debug_img)

    # === æ­¥é©Ÿ 3.5: æ•´æ¬„ä¸€è‡´æ€§å¯©æ ¸ï¼ˆé˜²æ­¢æŠŠç©ºç™½åˆ—ç¡¬å¡æˆä¸€å­—ï¼‰ ===
    COLUMN_MIN_RATIO = 0.3
    for i, char_label in enumerate(final_labels):
        if char_label == "?": continue
        lx = first_row_boxes[i][0]
        column_boxes = [b for b in practice_boxes if abs(b[0] - lx) < 50]

        nonblank_cnt = 0
        for (px, py, pw, ph) in column_boxes:
            roi = image[py:py+ph, px:px+pw]
            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            if not is_grid_blank_dynamically(gray_roi):
                nonblank_cnt += 1

        ratio = nonblank_cnt / len(column_boxes) if column_boxes else 0
        if ratio < COLUMN_MIN_RATIO:
            final_labels[i] = "?"
            
    # === (v16.9.0) æ­¥é©Ÿ 3.6: è¨ˆç®—å¯å®šå€çš„ç·´ç¿’æ ¼ç¸½æ•¸ (ç”¨æ–¼ç²¾ç¢ºçµ±è¨ˆ) ===
    for i, char_label in enumerate(final_labels):
        if char_label != "?":
            lx = first_row_boxes[i][0]
            column_boxes_count = len([b for b in practice_boxes if abs(b[0] - lx) < 50])
            total_addressable_grids += column_boxes_count

    # === æ­¥é©Ÿ 4: åˆ‡å‰²èˆ‡å„²å­˜ ===
    # æ ¸å¿ƒé‚è¼¯ï¼šæ­¤è¿´åœˆåƒ…è™•ç† final_labels ä¸­è¢«æˆåŠŸæ¨™è¨»çš„å­—å…ƒã€‚
    # è‹¥æ¨™ç±¤æ ¼åœ¨å‰é¢æ­¥é©Ÿä¸­è¢«åˆ¤å®šç‚ºç©ºç™½ (final_labels[i] == "?")ï¼Œ
    # å‰‡ä¸‹æ–¹çš„ 'continue' æŒ‡ä»¤æœƒç›´æ¥è·³éè©²æ¬„ï¼Œé”æˆã€Œæ¨™ç±¤æ˜¯ç©ºç™½å‰‡æ•´æ¬„ä¸å­˜ã€çš„éœ€æ±‚ã€‚
    for i, char_label in enumerate(final_labels):
        if char_label == "?": continue
        lx = first_row_boxes[i][0]
        print(f"                [è™•ç† '{char_label}' æ¬„ä½]")
        os.makedirs(os.path.join(OUTPUT_DIR, char_label), exist_ok=True)
        if char_label not in char_counters:
            char_counters[char_label] = 0
        column_boxes = [b for b in practice_boxes if abs(b[0] - lx) < 50]
        saved_count = 0
        
        for grid_idx, (px, py, pw, ph) in enumerate(column_boxes):
            roi = image[py:py+ph, px:px+pw]
            gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            
            if not is_grid_blank_dynamically(gray_roi):
                char_counters[char_label] += 1
                output_filename = f"{char_counters[char_label]:03d}.png"
                output_path = os.path.join(OUTPUT_DIR, char_label, output_filename)
                is_success, buffer = cv2.imencode('.png', roi)
                if is_success:
                    with open(output_path, 'wb') as f: f.write(buffer)
                    saved_count += 1
            else:
                total_blanks_skipped += 1
        
        if saved_count > 0:
            total_handwriting_saved += saved_count
            print(f"                              -> '{char_label}' å„²å­˜ {saved_count} å€‹å­—è·¡")
            if saved_count < 10:
                incomplete_columns_log.append({'page': page_filename, 'char': char_label, 'count': saved_count})
                print(f"                              -> â— è­¦å‘Š: '{char_label}' æ¬„å„²å­˜æ•¸é‡ä¸è¶³ ({saved_count}/10)ï¼Œå·²è¨˜éŒ„ã€‚")

print("\nâœ” å…¨éƒ¨é é¢è™•ç†å®Œæˆï¼")

# --- ä½å­˜é‡æ¬„ä½å ±å‘Š ---
if incomplete_columns_log:
    print("\n" + "="*50)
    print("--- âš ï¸ ä½å­˜é‡æ¬„ä½å ±å‘Š (å„²å­˜æ•¸é‡ < 10) ---")
    print("="*50)
    sorted_log = sorted(incomplete_columns_log, key=lambda x: x['page'])
    for log in sorted_log:
        print(f"  - é é¢: {log['page']:<15} | å­—å…ƒ: '{log['char']}' | å„²å­˜æ•¸é‡: {log['count']}/10")

    total_missing_chars = sum(10 - log['count'] for log in incomplete_columns_log)
    print("-" * 20)
    print(f"  -> ç¸½å…±ç¼ºå°‘ {total_missing_chars} å€‹å­—è·¡ã€‚")
    print("="*50)
else:
    if total_pages_processed > 0:
        print("\nâœ” æ­å–œï¼æ‰€æœ‰æˆåŠŸè™•ç†çš„æ¬„ä½å‡å„²å­˜äº† 10 å€‹æˆ–ä»¥ä¸Šçš„å­—è·¡ã€‚")

# --- æœ€çµ‚æˆæœçµ±è¨ˆå ±å‘Š ---
print("\n" + "="*50)
print("--- æœ€çµ‚æˆæœçµ±è¨ˆå ±å‘Š ---")
print("="*50)
if total_pages_processed > 0:
    print(f"ç¸½å…±è™•ç†é é¢æ•¸é‡: {total_pages_processed} é ")
    print(f"ç¸½å…±æ‰¾åˆ°çš„æ ¼å­æ•¸(å«æ¨™ç±¤+ç·´ç¿’): {total_grids_found} å€‹")
    print(f"  - æ¨™ç±¤åˆ—æ ¼å­æ•¸: {total_label_boxes_found} å€‹")
    print(f"  - ç·´ç¿’æ ¼ç¸½æ•¸ (ç‰©ç†åµæ¸¬): {total_practice_grids_found} å€‹")
    print("-" * 20)
    
    unresolved_labels = total_label_boxes_found - total_labels_recognized
    print(f"æˆåŠŸæ±ºå®šçš„æ¨™ç±¤æ¬„ä½æ•¸: {total_labels_recognized} æ¬„")
    if unresolved_labels > 0:
        print(f"æœªèƒ½æ±ºå®šçš„æ¨™ç±¤æ¬„ä½æ•¸: {unresolved_labels} æ¬„")
    
    unaddressable_grids = total_practice_grids_found - total_addressable_grids
    print(f"å¯å®šå€ç·´ç¿’æ ¼ç¸½æ•¸ (æœ‰æ¨™ç±¤): {total_addressable_grids} æ ¼")
    if unaddressable_grids > 0:
         print(f"ä¸å¯å®šå€ç·´ç¿’æ ¼ç¸½æ•¸ (æ¨™ç±¤æœªçŸ¥): {unaddressable_grids} æ ¼")

    print("-" * 20)
    print(f"--- åœ¨ {total_addressable_grids} å€‹ã€Œå¯å®šå€ã€æ ¼å­ä¸­çš„åˆ†æ ---")
    print(f"âœ… **æœ€çµ‚æˆåŠŸå„²å­˜çš„å­—è·¡ç¸½æ•¸**: {total_handwriting_saved} å¼µ")
    print(f"   - å› ã€Œç©ºç™½ã€è€Œè·³éçš„æ ¼å­æ•¸: {total_blanks_skipped} å€‹")
    
    total_non_blank_addressable_grids = total_addressable_grids - total_blanks_skipped
    print(f"   - å¯¦éš›æœ‰å­—è·¡çš„æ ¼å­ç¸½æ•¸: {total_non_blank_addressable_grids} å€‹")
    
    print("-" * 20)
    
    # æŒ‡æ¨™ä¸€ï¼šå„²å­˜ç‡ (è¡¡é‡ç¨‹å¼åŸ·è¡Œæ•ˆç‡)
    if total_non_blank_addressable_grids > 0:
        storage_rate = (total_handwriting_saved / total_non_blank_addressable_grids) * 100
        print(f"**å­—è·¡å„²å­˜ç‡ (å·²å„²å­˜ / å¯¦éš›æœ‰å­—è·¡)**: {storage_rate:.2f}%")
    else:
        print("**å­—è·¡å„²å­˜ç‡**: N/A (æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰å­—è·¡çš„å¯å®šå€æ ¼å­)")
    
    # æŒ‡æ¨™äºŒï¼šè³‡æ–™ç”¢å‡ºç‡ (è¡¡é‡ç›¸å°æ–¼ç†è«–å€¼çš„æ•ç²èƒ½åŠ›)
    total_grids_theory_practice = GRIDS_PER_PAGE_THEORY * total_pages_processed
    denominator_yield = total_grids_theory_practice - total_blanks_skipped - unaddressable_grids
    if denominator_yield > 0:
        yield_rate = (total_handwriting_saved / denominator_yield) * 100
        print(f"**è³‡æ–™ç”¢å‡ºç‡ (å·²å„²å­˜ / (ç†è«–ç¸½æ•¸ - ç©ºç™½ - æœªçŸ¥æ¨™ç±¤))**: {yield_rate:.2f}%")
    else:
        print("**è³‡æ–™ç”¢å‡ºç‡**: N/A (åˆ†æ¯ç‚ºé›¶æˆ–è² æ•¸)")

else:
    print("æ²’æœ‰è™•ç†ä»»ä½•é é¢ï¼Œç„¡æ³•ç”¢ç”Ÿå ±å‘Šã€‚")
print("="*50)